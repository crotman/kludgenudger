---
title: "Nudging for less kludges: focusing on PMD alerts as possible kludges: open, fixed or new?"
author: "Bruno Crotman"
header-includes:
  - \usepackage{lscape}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}  
  - \definecolor{darkred}{RGB}{150, 40, 40}
  - \definecolor{darkgreen}{RGB}{30, 120, 30}
  - \definecolor{darkorange}{RGB}{40, 40, 160}
  - \newcommand{\comentario}[1]{}
output: 
    pdf_document:
        number_sections: true
        fig_caption: true
        toc: true
        toc_depth: 3 
        keep_tex:  true
        
bibliography: bib.bib


---



```{r setup, include=FALSE}

library(xml2)
library(tidyverse)
library(gt)
library(knitr)
library(kableExtra)
library(tidygraph)
library(ggraph)
library(patchwork)
library(magrittr)
library(scales)
library(magrittr)
library(patchwork)
library(feather)
library(ggrepel)

knitr::opts_chunk$set(echo = FALSE, size = "small", warning = FALSE, message = FALSE, cache = FALSE, fig.pos="H")

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})


map_rule_small <- tribble(
    
    ~rule,                              ~small_rule,
    "class_or_interface_body",          "class_body",                   
    "class_or_interface_declaration",   "class_decl",
      "class_or_interface_type",          "class_type",
    "compilation_unit",                 "unit",
    "extends_list",                     "extends",
    "implements_list",                  "implements",
    "import_declaration",               "import",
    "method",                           "method",
    "name",                             "name",
    "package",                          "package",
    "type_declaration",                 "type_decl",
    "constructor_declaration",          "constructor",
    "field_declaration",                "field",
    "variable_id",                      "var_id",
    "formal_parameter",                 "param",
    "formal_parameters",                "params",
    "annotation",                       "annotation",
    "block",                            "block",
    "statement",                        "statement",
    "if_statement",                     "if"

)

 

```



```{r definitions, echo=FALSE}

size_line_of_code <- 160

length_alert_name <- 35

length_alert_name_side_by_side <- 14

size_line_of_code_side_by_side <- 77


pmd_path <- "pmd/bin/pmd.bat"

rule_path <- "rulesets/java/quickstart.xml"

output_path <-  ""  

examples <- tribble(
    
    ~name,                  ~path,                                                              ~output,          
    "Versão Old Original",  "old" ,  "old_original",
    "Versão New 1",         "new",   "new_1"
    
) %>% 
    mutate(id = row_number())

```


\section{Introduction}\label{intro}

This document is part of a research project about software degradation caused by careless developers' behavior and about strategies to deal 
with such undesired behavior. 
The strategies to deal with this problem will possibly be inspired by concepts from game theory.

We assume that software degradation can be measured by the number and the types of kludges made by software developers in the code.
One of the goals of this project is to study how software projects evolve in terms of number and kinds of kludges.
So far, we are trying to identify kludges by looking at alerts generated by the PMD source code analyzer.

To evaluate how the number of alerts evolve throughout the history of a	%In Section \ref{as_whole} we present a brief summary of the broader project that we intend to develop. In Section \ref{pmd}, we describe how we use PMD source code analyzer in two tasks:

%In Section \ref{as_whole} we present a brief summary of the broader project that we intend to develop. In Section \ref{pmd}, we describe how we use PMD source code analyzer in two tasks:

%In Section \ref{alg}, we describe the algorithm that help to categorize the alerts as \textbf{new}, \textbf{fixed} or \textbf{open}.

%In Section \ref{results}, we run the algorithm on some versions of the project ArgoUML and present some results. We compare the incidence of new PMD alerts found by the algorithm with the incidence of new comments that match to patterns related to Self Admitted Technical Debts.

% =========================================================
%
% PMD SOURCE CODE ANALYZER
%
% =========================================================

\section{PMD Source Code Analyzer}\label{pmd}

PMD is static source code analyzer that is commonly used to find possible programming flaws. We use PMD for two tasks: 

\begin{itemize}
\item To list the alerts that represent possible kludges. PMD receives a source code as input and generates a list of bad programming practices 
contained in the code, i.e., the alerts. The process we follow to generate the alerts using PMD source code analyzer is discussed in Section \ref{history};

\item To create an Abstract Syntax Tree (AST) from a source code with selected nodes. This will help us in the algorithm described in Section \ref{alg}. The creation of the AST using PMD is described in Section \ref{ast}.
\end{itemize}

PMD traverses the AST of a source code searching for violations of rules which are configured by the user. PMD comes with a default rule set for
the Java programming language. The default rule set finds common programming flaws such as unused variables, empty catch blocks, unnecessary object creation, and so forth. It is possible to configure a different set of rules by creating a custom XML file. In Figure \ref{simple_code}, we can see an example of a simple code and the alerts that were generated by the default rule set of PMD alerts tool.



```{r}

saida_alg2 <- kludgenudger::calculate_features_from_versions(
  code_file_old = "little-tree/code.java",
  code_file_new = "little-tree-new/code.java",
  pmd_path = pmd_path,
  glue_string = "{.data$id_alert}:line:{.data$beginline},{.data$small_rule}. alert:{if_else(is.na(.data$rule_alert),'none',.data$rule_alert)}",
  mostra_new = c(10, 15, 18, 16, 20, 3),
  mostra_old = c(10, 42,  15,  18, 3) 
    
)



```


```{java code old simple, code=kludgenudger::read_and_decorate_code_and_alerts("little-tree/code.java",   saida_alg2$versions_executed$pmd_output[[1]], FALSE, 10), echo = TRUE }

```


![Simple code with its alerts \label{simple_code}](figures/fake.png)


\subsection{Using PMD to capture the history of alerts}\label{history}

To evaluate how the number of alerts evolved throughout the history of a software project, we must be able to analyze two different versions of a source code (an old and a new version) and categorize each alert contained in the new version as either \textbf{new}, \textbf{fixed} or \textbf{open}.

We define a PMD alert generated for the old version as either **open** or **fixed** in the new version. 
An **open** alert remains in the new version of the code. 
A **fixed** alert does not exist in the new version. 

A PMD alert generated for the new version is either **open** or **new**. 
An **open** alert indicates that the same alert was identified in the old version of the source code. 
A **new** alert implies that the same alert cannot be identified in the old version. 

The intersection between \textbf{fixed} alerts, \textbf{new} alerts and \textbf{open} alerts is empty. The alerts identified as \textbf{open} are equivalent in both new and old versions. 
To decide whether an alert is \textbf{open}, \textbf{fixed} or \textbf{new}, one has to identify if an alert in the old version is equivalent to an alert in the new version. 
This document describes the algorithm we propose to make this classification. 
At this point we use the default rule set to generate the alerts.

\subsection{Using PMD to generate an Abstract Syntax Tree}\label{ast}

The AST is used to understand the location of an alert in a version of a code. We use this information in the algorithm described in Section \ref{alg}.

PMD traverses the source code visiting many different kinds of elements comprising the AST. If we build our own rules, aimed to capture only some
kinds of elements, we will generate list of ``alerts'' that will contain all the elements of the chosen kinds contained in the AST.


\section{PMD Source Code Analyzer}\label{pmd}

PMD is static source code analyzer that is commonly used to find possible programming flaws. 
In this work we use PMD for two tasks: to generate alerts that we interpret as clues about kludges and to create an AST from the source code with selected kinds of node.

\subsection{Using PMD Source Code Analyzer to generate alerts}\label{pmd_alerts}

PMD traverses the AST of a source code searching for violations of rules which are configured by the user. 
PMD comes with a default rule set for Java language. 
The default rule set finds common programming flaws like unused variables, empty catch blocks, unnecessary object creation, and so forth. 
It´s possible to configure a different set of rules creating a custom XML file. 
At this point we use the default rule set to generate the alerts that we interpret as kludges and try to categorize by the algorithm described in Section \ref{alg}

\newpage

In Figure \ref{simple_code}, we can see an example of a simple code and the alerts that were generated by the default rule set of PMD alerts tool.


\subsection{Using PMD to generate an Abstract Syntax Tree }\label{ast}

The AST is used, in conjunction with the relation between the lines we will see in Section \ref{map}, to understand the location of an alert in a version of a code. 
We use this information in the algorithm described in Section \ref{alg2}. The tree helps us in making matches between the old and the new versions that are not exact. Rather then betting on a match that depends solely on the location of the node where the alert is, we can get the information that the alerts are not in the same line but belong to methods that are in the same line, for instance.

We can see that the tool traverses the source code visiting many different kinds of elements. 
If we build our own simple rules, aimed only to capture some kinds of elements, we will generate list of "alerts" that will contain _all_re the elements of the chosen kinds contained in the AST.

\newpage

We do not use all the types of nodes recognized by PMD Alert to generate the AST because there are many kinds of nodes and some of them are redundant for our purposes. If we used all the kinds of nodes, we would end up with a tree that would not add value to our analysis but would add compelexity to our algorithm. The kinds of elements that were selected are the following ones:

\begin{itemize}

\item \textbf{Annotation}: a syntatic metadata added to a source code;

\item \textbf{Block}: a block of statements enclosed by braces;

\item \textbf{ClassOrInterfaceBody}: the body of an interface or a class, excluding the declaration;

\item \textbf{ClassOrInterfaceDeclaration}: class or interface, including the declaration and the body;

\item \textbf{ClassOrInterfaceType}: declaration of a type;

\item \textbf{CompilationUnit}: the root of an AST tree;

\item \textbf{ConstructorDecaration}: class or interface, including the declaration and the body;

\item \textbf{ExtendsList}: list of extensions of a class;

\item \textbf{FieldDeclaration}: field declaration, including the type, name and possible assignment;

\item \textbf{FormalParameter}: a parameter of a method or constructor;

\item \textbf{FormalParameters}:  list of formal parameters of a method or constructor;

\item \textbf{IfStatement}: an if statements including its blocks and condition;

\item \textbf{ImplementsList}: list of implementations of an interface;

\item \textbf{Import}: a package imported;

\item \textbf{Method}: a method, including body and declaration;

\item \textbf{Name}: a named value, like a variable, a class or a package referenced in the code.

\item \textbf{Package}: the indication of the package to which the compilation unit belongs;

\item \textbf{Statement}: any statement, like an if statement or an assignment;

\item \textbf{TypeDeclaration}: any type declaration;

\item \textbf{VariableId}: the name of a variable in a variable declaration.
\end{itemize}

We follow three steps to recreate the AST:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Link each element \(a\) to the set of elements \(X\) that are fully
  contained between the begin line / begin column and end line / end
  column of element \(a\). We can construct a directed graph in which
  the elements are the nodes and the links described are the edges. This
  is not a tree yet, because each node will have edges directed to all
  its descendants and not only its children in the AST.

\item
  Sort the nodes in the decreasing order of its number of children. The
  objective is to establish that, in a search through this graph, the
  first child chosen will be the one that is a child in the AST, and not
  only a descendant.

\item
  Proceed a deep-first search starting from the compilation unit node.
\end{enumerate}



%
% MARCIO: eu não entendi o objetivo do algoritmo acima. Você quer montar a AST somente
% com os tipos de nós que listou acima? Por quê? Justifique. Depois disso, quer recriar
% o código-fonte a partir desta AST parcial? Porque precisa disso?
%

%
% BRUNO: A quantidade de tipos de nós é gigantesca. A árvore fica gigantesca e muito redundante. A ideia é permitir matches que não sejam puramente baseados % na localização das linhas. Expliquei no texto. Às vezes pode não bater exatamente a localização, mas o nó método pai pode bater, portanto, podemos 
% considerar que os nós estão no mesmo método e se eles forem de mesmo tipo, com o mesmo alerta, debaixo do mesmo método... se tiver muita evidência de que % é a mesma coisa, podemos considerar que é. Aquela história: tem focinho de porco, orelha de porco, pé de porco. É porco? Só não sei como escrever isso 
% para um inglês...
%
%






\section{The research project}
\label{as_whole}

The aim of this section is to keep track of some discussions related to the project in a broader sense.

\subsection{Research project stages}
\label{research-stages}

These are the planned steps of the
project:

- Confirm the assumption that the frequency of PMD Alerts is an accurate measure of the prevalence of kludges;

- Confirm the assumption that kludges harm software development;

- Confirm the assumption that there is a game in which, in Nash equilibrium, a developer chooses a strategy in which he gets personal benefits while causing harm to the project, by making kludges;

- If all these assumptions are true, use mechanism design to devise how we can change the environment in a way that developers do not choose to make so much kludge, increasing the quality of the project in the long run.

- Implement this mechanism building a plugin for a prominent CI tool, such as Travis or Jenkins or GitLab.

\subsection{Kludge definition}\label{kludge definition}

A kludge is code that 

1. Partially fixes a bug or partially implements a feature. 

The term partial can be understood as in \textit{partial functions}. A partial function is undefined for some elements in the formal domain. For instance, the square root function restricted to the integers: $f(25)$ is defined, but $f(26)$ is undefined. In terms of features, we can think about a developer calculating the point on which two lines cross and neglecting the case of parallel lines

2. The developer knows that the code is only a partial solution, with high probability. 

It is necessary to study technical debt papers to enrich the conceptual background.

\subsection{Research questions}

\noindent
\textbf{RQ: Is the frequency of PMD Alerts an accurate measure of the prevalence of kludges?} %\label{PMD_Kludge}

We have to identify events with an intense introduction of PMD Alert. To do this we have to be able to categorize open, new and fixed PMD Alerts.

For each pair of an old and a new version of a source code, we can measure the intensity of possible kludge introduction occurred. This evidence of possible kludge introduction must be normalized by the size of the change in source code between the two versions. The measure may follow a formula like:


%
% MARCIO: você pode medir o "número de alertas". No texto, você está trocando este
% termo por kludge, mas esta relação é justamente o que você quer provar.
%

%
% BRUNO: Sim, agora eu chamei de possible kludges.
%



$$ \frac{\#NewAlerts - \#FixedAlerts}{Change}    $$

or

$$ \frac{\#NewAlerts - \#FixedAlerts}{\#NewAlerts + \#FixedAlerts}    $$


With these events identified, including their location in the code and their position in the time line of commits, we can measure the correlation of these events with some other evidence of kludge. In Section \ref{results} we calulate these metrics for 


%
% MARCIO: o que seria "their moment"? Uma versão em que foram introduzidos?
% se você está preocupado com a introdução de issues, porque está tratando
% os issues resolvidos (no numerador) nas fórmulas acima?
%

% 
%BRUNO: Na primerira fórmula eu vou usar uma medida baseada no número de linhas, na segunda realmente eu uso o número total de alertas. É uma proporção de
% de quantos são os novos alertas em relação ao núymero total de alertas. O número total de alertas fechados e abertos serviria como "tamanho da mudança".
% É ruim?
%
%


An evidence could be churns around the location of the event in subsequent commits. And this would be related with the statement 1 of the definition of kludge in Section \ref{roadmap}.

Other possible evidence could be survey and bag of words mining of issue, mailing lists, and commit messages showing evidence of kludge game. 
This could be related more with the Statement 2 of the definition of kludge in Section \ref{roadmap}.

\vspace{32px}
\textbf{RQ: Do kludges harm software development?} \label{kludge_harm}

We need some way to measure degradation after a heavy introduction of kludges. A drop in the popularity may not be a proper evidence. The 
increment in the number of issues and bug fixed nor necessarily represent a degradation. Churn could be used here.


% =========================================================
%
% ALGORITHMS
%
% =========================================================



\section{Algorithm to categorize alerts}\label{alg}

This Section discusses the algorithm to categorize alerts, which uses the AST to create features that help to infer if two alerts in different
versions must be considered the same. This algorithm does not return a single categorical answer for each alert (new, open or fixed), but features that must be evaluated by some heuristic or possibly a statistical learning tool.

In Section \ref{source_used} the source code used in this description is shown. Section \ref{map} describes how the source code lines are mapped. In the Section \ref{feature_creation}, we show how the features are created. 
Even though it is not possible to be sure if a pair alerts in different versions are same alert, we think that these features can provide some clues. The heuristic we used for this work is described in Section \ref{heuristic}.


%
% MARCIO: retorna features? o que são estas features?
%

%
% BRUNO: o pessoal de aprendizado estatístico que chama de feature cada tipo de dado usado como entrada nos modelos deles. Tem até uma área de feature 
% engineering, que serve pra gerar, a partir dos dados brutos, esses dados de entrada que facilitam a vida do modelo. Por exemplo, ao invés de enfiar num 
% modelo uma imagem de um exame, vc pode fazer primeiro um tratamento que extrai o diâmetro médio das células que ele encontra na imagem. Seria o paralelo 
% de em vez de meter no nosso modelo (que no caso é uma heurística) o código-fonte todo, a gente meter essas features pra que seja mais fácil definir a 
% classificação do alerta. Vc acha que vale a pena eu escrever essas coisas no texto?


\newpage

\blandscape

\subsection{Source code used}\label{source_used}

In this Section, we will consider the old and new version of a source code as presented in Figure
\ref{old_simple_code} and the new version in Figure
\ref{new_simple_code}. In the new version, the alert generated in the
line 11 of the old version was fixed.

```{java showing codes, code=kludgenudger::read_and_decorate_code_and_alerts_mapped("little-tree/code.java", saida_alg2$versions_executed$pmd_output[[1]], "little-tree-new/code.java", saida_alg2$versions_executed$pmd_output[[2]],saida_alg2$versions_crossed$lines_map[[1]], TRUE, 20), echo=TRUE, size="scriptsize"  }
```

![Source code used in the description of the algorithm](figures/fake.png)

\elandscape    

\newpage 



\subsection{Using information from git diff, create a relation between the lines}\label{map}


For each difference stated in the output (the sections of the diff file starting with "@@"), there is an indication of the number of lines removed from the old version and the number of lines added to the new one.
The line in which the lines are removed from the old version and the line at which the lines are added is indicated, too.  
By using this information we create a relation between the lines of the old version and the equivalent lines in the new version.
For the new and old versions presented in Section \ref{codes}, the relation is shown in Table \ref{table_map}.


```{r showing map  }
saida_alg2$versions_crossed$lines_map[[1]] %>% 
  ungroup() %>% 
  mutate(
    row = row_number(),
    na_mark = if_else(is.na(map_remove) | is.na(map_add), row , NA_integer_  ),
    next_na = na_mark,
    last_na = na_mark
  ) %>% 
  fill(
    next_na, .direction = "up"
  ) %>% 
  fill(
    last_na, .direction = "down"
  ) %>% 
  replace_na(
    list(
      last_na = 0,
      next_na = nrow(saida_alg2$versions_crossed$lines_map[[1]]) + 1
    )
  ) %>%
  mutate(
    dist_next = next_na - row,
    dist_last = row - last_na + 0.1
  ) %>%
  rowwise() %>% 
  mutate(
    min_dist = min(dist_next, dist_last)
  ) %>% 
  filter(
    min_dist < 4
  ) %>%
  ungroup() %>% 
  mutate(    
    map_remove = 
      case_when(
        min_dist == 3.1 ~ str_glue("{lag(map_remove)+1}-"),
        min_dist == 3.0 ~ str_glue("-{lead(map_remove)-1}"),
        TRUE ~ map_remove %>% as.character()
      ),
    map_add = 
      case_when(
        min_dist == 3.1 ~ str_glue("{lag(map_add)+1}-"),
        min_dist == 3.0 ~ str_glue("-{lead(map_add)-1}"),
        TRUE ~ map_add %>% as.character()
      )
  ) %>% 
  select(old = map_remove, new = map_add) %>% 
  mutate(  
    old = if_else(is.na(old), str_glue("\\textcolor{{white}}{{{row_number()}}}"), old),
    new = if_else(is.na(new), str_glue("\\textcolor{{white}}{{{row_number()}}}"), new)
  ) %>% 
  pivot_wider(
    names_from = old,
    values_from = new,
    names_repair = "minimal"
  ) %>% 
  kable(
    caption = "Relation between lines of the old version and lines of the new version\\label{table_map}",
    escape = FALSE
  ) %>% 
  kable_styling(
    font_size = ,
    latex_options = c("scale_down")
  )
```






\subsection{Feature engineering} \label{feature_creation}

In Figure \ref{AST_compare_id_alerts} we can see the ASTs for the old and the new versions. We can see the kind of nodes and the PMD alert if there is one. In this figures, the numbers in the nodes are meaningless and are presented only for reference.


```{r, echo=FALSE, message=FALSE, warning=FALSE,  out.width="100%", fig.width=15, fig.height=15, fig.cap="Abstract Syntax Trees. New and old versions, with alerts \\label{AST_compare_id_alerts}", fig.pos="H"}

chart_graph_new <- kludgenudger::show_ast(
  saida_alg2$graph_new_with_alert,
  size_label = 5,
  show_label = TRUE,
  alpha_label = "mostra",
  name_field = "glue",
  aspect = 2

)

chart_graph_old <- kludgenudger::show_ast(
  saida_alg2$graph_old_with_alert,
  size_label = 5,
  show_label = "TRUE",
  alpha_label = "mostra",
  name_field = "glue",
  aspect = 2

)



chart_graph_old + chart_graph_new


```

\begin{itemize}
\item
  Considering the relation between the lines of the two versions constructed as we saw in Section \ref{map}, the nodes in both trees begin and end in related lines;

\item
  The nodes are of the same kind.
\end{itemize}


```{r , echo=FALSE, message=FALSE, warning=FALSE,  out.width="100%", fig.width=15, fig.height=15, fig.cap="Abstract Syntax Tree. Nodes with the same number are equivalent \\label{AST_with_alerts}", fig.pos="H"}


chart_graph_old <- kludgenudger::show_ast(
  saida_alg2$graph_old_with_alert, 
  size_label = 5, 
  show_label = TRUE, 
  alpha_label = 1, 
  node_text_field = "id_group",
  name_field = "text_alert",
  aspect = 2
  
)


chart_graph_new <- kludgenudger::show_ast(
  saida_alg2$graph_new_with_alert, 
  size_label = 5, 
  show_label = TRUE, 
  alpha_label = 1, 
  node_text_field = "id_group",
  name_field = "text_alert",
  aspect = 2
  
)


chart_graph_old + chart_graph_new



```

The path between the alert and the root of the AST can be seen in \ref{AST_alert_1}. Comparing the path of different alerts is possible to determine if the nodes belong to the same method, for instance.


```{r, echo=FALSE, message=FALSE, warning=FALSE,  out.width="100%", fig.width=10, fig.height=3, fig.cap="Abstract Syntax Tree \\label{AST_alert_1}", fig.pos="H"}

kludgenudger::show_ast(
  saida_alg2$graphs_from_alerts_old %>%  rename( id_alert = id_alert_old, graph = graph_old) %$% graph[[1]] , 
  size_label = 4,
  aspect = 0.2
)


 


```

The algorithm generates a set of features for each pair of alerts $(n,o)$ with one element $n$ coming from the old version and one element $o$ coming from the new version. 
The features do not lead to a direct conclusion. 
It´s necessary to create a heuristic or statistical learning algorithm that will decide the final verdict based on the features.

We propose the following list of features:

* Same Rule: a boolean indicator that tells if the alerts are of the same type

* Same Group ID: a boolean indicator that tells if the alerts are equivalent as defined in Figure \ref{AST_groups}

* Same Method Group ID: a boolean indicator that tells if the alerts belong to the same method. We know the alert's method following the path from the alert´s node to the root. The first node of the kind "method" found in this path defines the alert's method. If this is the same for $o$ and for $n$, then they belong to the same method.

* Same Method Name: a boolean indicator that tells if the alerts were found in a method with the same name.

* Same Block: a boolean indicator that shows if the $o$ and $n$ belong to the same block. It is defined the same way the "Same method" indicator is defined.

* Same Code: a boolean indicator that shows the nodes that generate the alert have the same programming code.

* Same Method Code: a boolean indicator that shows that the methods that contain the nodes that generate the alert have the same programming code.

* Line distance: $o$ and $n$ have a begin line $b(o)$ and $b(n)$ and an end line $e(n)$ and $e(n)$. Line distance is $abs(mean(b(o), e(o)) - mean(b(n), e(n)))$

* Normalized line distance (block size): this is the line distance but normalized by the size of the last common node.

* Normalized line distance (method size): this is the line distance but normalized by the size of the last common method (if there is no common method, it´s normalized by the side of the compilation unit).

* Normalized line distance (compilation unit size): this is the line distance but normalized by the size of the compilation unit.

Table \ref{table_features} shows the combinations $(n,o)$ in the example. There are $2 \cdot 1 = 2$ combinations whereas we have two alerts in the old version and one alert in the new one.  


```{r}

kludgenudger::report_features(saida_alg2, "Resulting features\\label{table_features} ")
  
```



```{r}

features_rename <- kludgenudger::calculate_features_from_versions(
  code_file_new = "data/rename_method/new/code.java" ,
  code_file_old = "data/rename_method/old/code.java",
  pmd_path = pmd_path
)

```

\newpage

\blandscape

\subsection{Example: Renaming method} \label{example_rename_method}

In this example, the new and old versions have only one alert. The method in which the alert happens is renamed from MethodX to methodZ.

```{java showing codes, code=kludgenudger::read_and_decorate_code_and_alerts_mapped("data/rename_method/old/code.java", features_rename$versions_executed$pmd_output[[1]], "data/rename_method/new/code.java", features_rename$versions_executed$pmd_output[[2]],features_rename$versions_crossed$lines_map[[1]], TRUE, 20), echo=TRUE, size="scriptsize"  }
```

![Comparison between old and new version \label{comparison_rename}](figures/fake.png)

\elandscape    

\newpage 


In the Table \ref{features_rename} we can see that the features "Same Method Name" and "Same Method Group ID" are now FALSE.

```{r}

kludgenudger::report_features(features_rename, "Resulting features: rename method example \\label{features_rename} ")


```


\newpage

\blandscape


\subsection{Example: including a statement before} \label{example_including_statement}


```{r}

include_statement_before <- kludgenudger::calculate_features_from_versions(
  code_file_new = "data/include_statement_before/new/code.java" ,
  code_file_old = "data/include_statement_before/old/code.java",
  pmd_path = pmd_path
)

```


In this example, a new statement is included before the alert.

```{r}

code <- kludgenudger::read_and_decorate_code_and_alerts_mapped("data/include_statement_before/old/code.java", include_statement_before$versions_executed$pmd_output[[1]], "data/include_statement_before/new/code.java", include_statement_before$versions_executed$pmd_output[[2]],include_statement_before$versions_crossed$lines_map[[1]], TRUE, 20)

```


```{java showing codes, code=code, echo=TRUE, size="scriptsize"  }
```

![Comparison between old and new version \label{comparison_include_statement_before}](figures/fake.png)

\elandscape    

\newpage 


In the Table \ref{include_statement_before} we can see that the features are not affected by this new statement.

```{r}

kludgenudger::report_features(include_statement_before, "Resulting features: statement included before \\label{include_statement_before} ")


```

\newpage

\blandscape


\subsection{Example: nesting the alert in an if statement} \label{example_nested_in_other_if}

```{r}

nested_in_other_if <- kludgenudger::calculate_features_from_versions(
  code_file_new = "data/nested_in_other_if/new/code.java" ,
  code_file_old = "data/nested_in_other_if/old/code.java",
  pmd_path = pmd_path
)

```



```{java showing codes, code=kludgenudger::read_and_decorate_code_and_alerts_mapped("data/nested_in_other_if/old/code.java", nested_in_other_if$versions_executed$pmd_output[[1]], "data/nested_in_other_if/new/code.java", nested_in_other_if$versions_executed$pmd_output[[2]],nested_in_other_if$versions_crossed$lines_map[[1]], TRUE, 20), echo=TRUE, size="scriptsize"  }
```

![Comparison between old and new version \label{comparison_nested_in_other_if}](figures/fake.png)

\elandscape    

\newpage 


In the Table \ref{nested_in_other_if} we can see that the algorithm does not recognize the two nodes as equivalent, but other features can lead us to the conclusion that the alert is still open.

```{r}

kludgenudger::report_features(nested_in_other_if, "Resulting features: statement included before \\label{nested_in_other_if} ")


```


\newpage

\blandscape


\subsection{Example: editing the line that generates the alert} \label{example_editing_line}

```{r}

editing_line <- kludgenudger::calculate_features_from_versions(
  code_file_new = "data/editing_line/new/code.java" ,
  code_file_old = "data/editing_line/old/code.java",
  pmd_path = pmd_path
)

```




```{java showing codes, code=kludgenudger::read_and_decorate_code_and_alerts_mapped("data/editing_line/old/code.java", editing_line$versions_executed$pmd_output[[1]], "data/editing_line/new/code.java", editing_line$versions_executed$pmd_output[[2]],editing_line$versions_crossed$lines_map[[1]], TRUE, 20), echo=TRUE, size="scriptsize"  }
```


![Comparison between old and new version \label{comparison_editing_line}](figures/fake.png)

\elandscape    

\newpage


In the Table \ref{editing_line} the nodes are not recognized as equivalent, but other features can lead us to the conclusion that the alert is still open.

```{r}

kludgenudger::report_features(editing_line, "Resulting features: alert line edited \\label{editing_line} ")


```

\newpage

\blandscape


\subsection{Example: changing the order of the methods} \label{example_editing_line}


We changed the order of the methods in two ways


```{r}

changing_method_order <- kludgenudger::calculate_features_from_versions(
  code_file_new = "data/changing_method_order/new/code.java" ,
  code_file_old = "data/changing_method_order/old/code.java",
  pmd_path = pmd_path
)

```


```{java showing codes, code=kludgenudger::read_and_decorate_code_and_alerts_mapped("data/changing_method_order/old/code.java", changing_method_order$versions_executed$pmd_output[[1]], "data/changing_method_order/new/code.java", changing_method_order$versions_executed$pmd_output[[2]],changing_method_order$versions_crossed$lines_map[[1]], TRUE, 20), echo=TRUE, size="scriptsize"  }
```


![Comparison between old and new version \label{comparison_changing_method_order}](figures/fake.png)

\elandscape    

\newpage

In the Table \ref{changing_method_order} the nodes are not recognized as equivalent, but other features can lead us to the conclusion that the alert is still open.

```{r}

kludgenudger::report_features(changing_method_order, "Resulting features: changed methods order \\label{changing_method_order} ")


```

\newpage

\blandscape




```{r}

changing_method_order_2 <- kludgenudger::calculate_features_from_versions(
  code_file_new = "data/changing_method_order_2/new/code.java" ,
  code_file_old = "data/changing_method_order_2/old/code.java",
  pmd_path = pmd_path
)

```


```{java showing codes, code=kludgenudger::read_and_decorate_code_and_alerts_mapped("data/changing_method_order_2/old/code.java", changing_method_order_2$versions_executed$pmd_output[[1]], "data/changing_method_order_2/new/code.java", changing_method_order_2$versions_executed$pmd_output[[2]],changing_method_order_2$versions_crossed$lines_map[[1]], TRUE, 20), echo=TRUE, size="scriptsize"  }
```


![Comparison between old and new version \label{comparison_changing_method_order_2}](figures/fake.png)

\elandscape    

\newpage




In the Table \ref{changing_method_order_2} the nodes are not recognized as equivalent, but other features can lead us to the conclusion that the alert is still open.

```{r}

kludgenudger::report_features(changing_method_order_2, "Resulting features: changed methods order 2 \\label{changing_method_order_2} ")


```


\subsection{Heuristic to decide based on the features}\label{heuristic}


The heuristic chosen in this work follow these rules when :

\begin{itemize}
\item If all the boolean features are TRUE and **Line Distance** is true, then the new and old alerts are declared the same and open;
\item If **Same Method Code** is TRUE, then we consider it's the same method, even if the name or the group is not the same. But if the method code is the same, then the alert code and the kind of the alert must be the same. So if the features **Same Rule**, **Same Method Code** and 
**Same Code** are TRUE, then the new and old alerts are declared the same and open;
\item If it is the same id_group, then they are mapped to the same lines. 
If this happens, the kind of alert is the same and one of the features about the method is the same, then the new and old alerts are declared the same and open. 
The features about the method are all;
\end{itemize}



\section{Comparing new alerts with new SATD comments}\label{results}


In this section, we select some tagged versions of the project ArgoUML. 
For each pair of sequential versions, we generate the PMD Alerts and categorise them in New, Fixed and Open using the algorithm described in Section \ref{alg}.
We want to understand if the amount of new alerts, normalized by the magnitude of the overall change between two versions, is a good proxy for the amount of kludge introduced in the code base. 
A first approach we try in this preliminary investigation is to measure the correlation between the normalized amount of new alerts and comments that indicate Self Admitted Technical Debt.

In @Potdar2014, the authors discuss that the existence of comments that contain some specific patterns may indicate what they call Self-Admitted Technical Debts (SATD). 
In @Sierra2019, Self-Admitted Technical Debt is defined as the event in which the developer consciously introduce debt. 
According to these two works, the developer acknowledges the SATD in the form of comments. 
In @Wehaibi2016 we can find some patterns based on the work of Potdar and Shihab. 
For instance, some of these patterns are "hack", "retarded", "remove this code", "treat this as a soft error", "kludge", "fixme", "this isn't quite right", "fix this crap", "abandon all hope" and "kaboom".
 

In this document, we selected some versions from the project ArgoUML and extracted the PMD alerts, using the the procedure described in Section \ref{alg}. Figure \ref{timeseries} shows three metrics related to the transitions of versions. 

In the first plot, "Change" tries to measure the amount of difference between versions, summing the module of the difference between the number of lines of code of new and old files: 

\begin{equation} \label{eq_change} \sum_{f \in files}{|\#LoC_{f, new} - \#LoC_{f, old}|} \end{equation}

The second plot shows the number of new and fixed alerts, as categorized by the algorithm in Section \ref{alg}.

The third plot shows the number of comments that contain expressions listed in @Wehaibi2016. We categorize each comment as new, fixed and open using a simple approach: if the text in the comment is the same, the comments are classified as "open", the other ones are classified as fixed, if they are in the old version and open if they are in the new version. 



```{r}


# all_results <- read_rds("all_results.rds")
# 
# 
# categorised <- all_results %>%
#   mutate(data = map(.x = data, .f = function(x){x$categorised_alerts}  )) %>%
#   unnest(data) %>%
#   group_by(version_old, version_new) %>%
#   mutate(
#     id_file = row_number()
#   ) %>%
#   unnest(data)
# 
# versions_executed <- all_results %>%
#   mutate(data = map(.x = data, .f = function(x){x$versions_executed}  )) %>%
#   unnest(data) %>%
#   group_by(version_old, version_new) %>%
#   mutate(
#     id_file = row_number()
#   ) %>%
#   unnest(data)
# 
# write_rds(categorised, "categorised.rds")
# 
# write_rds(versions_executed, "versions_executed.rds")

categorised <-  read_rds("categorised.rds")

versions_executed <-  read_rds("versions_executed.rds")

friction <- versions_executed %>% 
  mutate(friction = abs(lines_right - lines_left)) %>% 
  select(
    version_old,
    version_new,
    friction
  ) %>% 
  group_by(
    version_old,
    version_new
  ) %>% 
  summarise(
    friction = sum(friction)
  ) %>% 
  mutate(
    comparison = str_glue("{version_old} to {version_new}")
  ) 



fixed_new_alerts <- categorised %>% 
  ungroup() %>% 
  select(
    version_old,
    version_new,
    category
  ) %>% 
  mutate(
    comparison = str_glue("{version_old} to {version_new}")
  ) %>% 
  filter(
    category %in% c("fixed", "new")
  ) %>% 
  group_by(
    comparison,
    category
  ) %>% 
  summarise(
    n = n()
  ) %>% 
  mutate(
    category = str_to_title(category)
  )



total_alerts <- categorised %>% 
  ungroup() %>% 
  select(
    version_old,
    version_new,
    category
  ) %>% 
  filter(
    category %in% c("fixed", "new")
  ) %>% 
  mutate(
    comparison = str_glue("{version_old} to {version_new}")
  ) %>% 
  group_by(
    comparison
  ) %>% 
  summarise(
    n = n()
  )



alerts_friction <- total_alerts %>% 
  inner_join(
    friction,
    by = c("comparison")
  )


# 
# satd_expressions <- c(
# "hack",
# "retarded",
# "at a loss",
# "stupid",
# "remove this code",
# "remove this",
# "ugly",
# "take care",
# "something's gone wrong",
# "something has gone wrong",
# "something gone wrong",
# "nuke",
# "is problematic",
# "problematic",
# "may cause problem",
# "hacky",
# "unknown why we ever experience this",
# "treat this as a soft error",
# "silly",
# "workaround for bug",
# "workaround",
# "kludge",
# "fixme",
# "this isn't quite right",
# "trial and error",
# "give up",
# "this is wrong",
# "hang our heads in shame",
# "temporary solution",
# "temporary fix",
# "causes issue",
# "something bad is going on",
# "cause for issue",
# "this doesn't look right",
# "this does not look right",
# "is this next line safe",
# "this indicates a more fundamental problem",
# "temporary crutch",
# "this can be a mess",
# "this isn't very solid",
# "this is temporary and will go away",
# "is this line really safe",
# "there is a problem",
# "some fatal error",
# "something serious is wrong",
# "don't use this",
# "do not use this",
# "get rid of this",
# "doubt that this would work",
# "this is bs",
# "give up and go away",
# "risk of this blowing up",
# "just abandon it",
# "prolly a bug",
# "buggy",
# "probably a bug",
# "hope everything will work",
# "toss it",
# "barf",
# "something bad happened",
# "fix this crap",
# "yuck",
# "certainly buggy",
# "remove me before production",
# "remove this before production",
# "you can be unhappy now",
# "this is uncool",
# "bail out",
# "it doesn't work yet",
# "it does not work yet",
# "crap",
# "inconsistency",
# "abandon all hope",
# "kaboom"
# )
# 
# satd_expressions <- str_glue("\\b{satd_expressions}\\b")
# 
# 
# acha_kludge <- function(x){
#   bateu <- str_match(string = x, pattern = satd_expressions)
#   bateu[!is.na(bateu)]
# }
# 
# 
# library(furrr)
# 
# plan(multiprocess)
# 
# # teste <- comments_18 %>%
# #   mutate(
# #     comment = str_to_lower(comment)
# #   ) %>%
# #   mutate(match = future_map(.x = comment, .f = acha_kludge, .progress = TRUE )) %>%
# #   unnest(match)
# 
# 
# comments_kludge <- list.files(path = "C:/doutorado/resultados", pattern = "rds", full.names =  TRUE ) %>%
#   enframe(
#     name = "id_version",
#     value = "file_version"
#   ) %>%
#   mutate(
#     comments = map(
#     .x = file_version,
#     .f = read_rds
#   )) %>%
#   unnest(comments) %>%
#   mutate(
#     comment = str_to_lower(comment)
#   ) %>%
#   mutate(match = furrr::future_map(.x = comment, .f = acha_kludge, .progress = TRUE )) %>%
#   unnest(match)
# 
# write_rds(comments_kludge, "comments_kludge.rds")


compare_comments  <- function(old, new){
  
  old <- old %>% 
    rename_with(
      ~str_glue("{.x}_old")
    )
  
  new <- new %>% 
    rename_with(
      ~str_glue("{.x}_new")
    )
  
  saida <- comments_comparison <- old %>% 
    full_join(new,
              by = c("comment_old" = "comment_new") )
  
  saida %>% 
    summarise(
      n_comments_new = id_comment_old %>% is.na() %>% sum(),
      n_comments_fixed = id_comment_new %>% is.na() %>% sum()
    )
  
}


comments_kludge <- read_rds("comments_kludge.rds") %>% 
  mutate(
    version = str_match(file_version, "[0-9]{2}_?[0-9]?")
  ) %>% 
  select(
    version,
    comment
  ) %>%
  mutate(
    id_comment = row_number()
  ) %>% 
  group_by(
    version
  ) %>% 
  nest() %>% 
  ungroup() %>% 
  mutate(
    comments_old = lag(data)
  ) %>% 
  rename(
    comments_new = data
  ) %>% 
  mutate(
    comparison = str_glue("{lag(version)} to {version}")
  ) %>% 
  slice_tail(
    n = nrow(.) - 1
  ) %>% 
  mutate(
    comparison_comments = map2(.x = comments_old, .y = comments_new, .f = compare_comments  )
  ) %>% 
  select(
    comparison,
    comparison_comments
  ) %>% 
  unnest(comparison_comments)


comparisons <- fixed_new_alerts %>% 
  pivot_wider(
    names_from = category,
    values_from = n
  ) %>% 
  rename(
    n_fixed_alerts = Fixed,
    n_new_alerts = New
  ) %>% 
  left_join(
    total_alerts,
    by = c("comparison")
  ) %>% 
  rename(
    n_alerts = n
  ) %>% 
  left_join(
    friction,
    by = c("comparison")
  ) %>% 
  left_join(
    comments_kludge,
    by = c("comparison")
  ) %>% 
  mutate(
    n_comments = n_comments_new + n_comments_fixed,
    prop_new_alerts = n_new_alerts / n_alerts,
    prop_new_comments = n_comments_new / n_comments,
    prop_new_alerts_friction = (n_new_alerts - n_fixed_alerts)/friction ,
    prop_new_comments_friction = (n_comments_new - n_comments_fixed)/friction
  ) 


comparisons_tidy <- comparisons %>% 
  select(-c(version_new, version_old)) %>% 
  pivot_longer(
    cols = -comparison,
    names_to = "atribute",
    values_to = "value"
  )



```



```{r, fig.asp= 1.75, fig.cap="\\label{timeseries}Changes, alerts and comments", fig.height=0.9 }


fixed_new_data <- comparisons_tidy %>% 
  filter(atribute %in% c("n_fixed_alerts", "n_new_alerts")) %>% 
  mutate(
    category = if_else(atribute == "n_fixed_alerts", "Fixed", "New")
  )


ggplot_fixed_new <- ggplot(fixed_new_data,
    aes(
      x = comparison,
      y = value,
      color = category,
      group = category
    ) 
) +
  geom_line(
    size = 1.2
  ) +
  geom_point(
    size = 2.5
  ) +
  theme_minimal() +
  scale_color_manual(
    values = c(Fixed = "darkgreen", New = "darkred") 
  ) +
  theme(
   axis.text.x = element_text(angle = 90) ,
   legend.position = "top"
  ) +
  scale_y_continuous(
    labels = number_format(big.mark = ",")
  ) +
  ggtitle(
    "Number of fixed/new alerts per version transition"
  ) +
  labs(
    x = "Transition",
    y = "Number of alerts",
    color = "Category"
  )


changed <- comparisons_tidy %>% 
  filter(atribute %in% c("friction")) 

ggplot_changed <- ggplot(changed,
    aes(
      x = comparison,
      y = value,
      group = 1
    )
) +
  geom_line(
    size = 1.2,
    color = "darkblue"
  ) +
  geom_point(
    size = 2.5,
    color = "darkblue"
  ) +
  theme_minimal() +
  theme(
   axis.text.x = element_text(angle = 90) ,
   legend.position = "top"
  ) +
  scale_y_continuous(
    labels = number_format(big.mark = ",")
  ) +
  ggtitle(
    "Change per version transition"
  ) +
  labs(
    x = "Transition",
    y = "Change"
  )
                          


fixed_new_comments <- comparisons_tidy %>% 
  filter(atribute %in% c("n_comments_new", "n_comments_fixed")) %>% 
  mutate(
    category = if_else(atribute == "n_comments_fixed", "Fixed", "New")
  )


ggplot_fixed_new_comments <- ggplot(fixed_new_comments,
    aes(
      x = comparison,
      y = value,
      color = category,
      group = category
    ) 
) +
  geom_line(
    size = 1.2
  ) +
  geom_point(
    size = 2.5
  ) +
  theme_minimal() +
  scale_color_manual(
    values = c(Fixed = "darkgreen", New = "darkred") 
  ) +
  theme(
   axis.text.x = element_text(angle = 90) ,
   legend.position = "top"
  ) +
  scale_y_continuous(
    labels = number_format(big.mark = ",")
  ) +
  ggtitle(
    "Number of fixed/new comments per version transition"
  ) +
  labs(
    x = "Transition",
    y = "Number of comments",
    color = "Category"
  )


ggplot_changed / ggplot_fixed_new / ggplot_fixed_new_comments + plot_layout(heights = unit(c(5, 5, 5), c("cm", "cm", "cm") ), widths = unit(c(10, 10, 10), c("cm", "cm", "cm") ))



```

\newpage 


We try to measure if there is a correlation between the amount of new alerts and the amount of new comments.

Figure \ref{scatter_prop} shows the relation between the proportion of new alerts and the proportion of new comments:

$$PropNewAlerts = \frac{NewAlerts}{NewAlerts + OldAlerts}$$


$$PropNewComments = \frac{NewComments}{NewComments + OldComments}$$

We can see that there is a positive correlation.


```{r fig.cap="\\label{scatter_prop}Proportion of new alerts x Proportion of new comments", fig.pos="H"}


ggplot(
  comparisons %>% filter(friction > 1000),
  aes(
    x = prop_new_comments,
    y = prop_new_alerts,
  )
  ) +
  geom_point( aes(size = friction)) +
  geom_text_repel(aes(label = comparison), nudge_y = 0.05, size = 2) +
  geom_smooth(method = "lm") +
  ggtitle(
    "Proportion of new comments x Proportion of new alerts"
  ) +
  labs(
    x = "Proportion of new comments",
    y = "Proportion of new alerts",
    size = "Change"
  ) +
  scale_x_continuous(
    label = percent_format()
  ) +
  scale_y_continuous(
    label = percent_format()
  ) +
  scale_size_continuous(
    label = number_format(big.mark = ",", accuracy = 1)
  ) +
  theme_minimal() +
  theme(
    legend.position = "top"
  ) +
  NULL

```
  

In Figure \ref{scatter_diff} we correlate two metrics based on the difference between the number of alerts and comments normalized by the amount of change:

$$DiffNewAlerts = \frac{NewAlerts - OldAlerts}{Change}$$

$$DiffNewComments = \frac{NewComments - OldComments}{Change}$$

Where Change is calculated as in Equation \ref{eq_change}.

We can see that there is a positive correlation too.


```{r  fig.cap="\\label{scatter_diff}Proportion of new alerts x Proportion of new comments", fig.pos="H"}

ggplot(
  comparisons %>% filter(friction > 1000),
  aes(
    x = prop_new_comments_friction,
    y = prop_new_alerts_friction,
  )
  ) +
  geom_point( aes(size = friction)) +
  geom_text_repel(aes(label = comparison), nudge_y = 0.05, size = 2) +
  geom_smooth(method = "lm") +
  ggtitle(
    "Normalized difference: comments x alerts"
  ) +
  labs(
    x = "Normalized difference between old and new comments",
    y = "Normalized difference between old and new alerts",
    size = "Change"
  ) +
  scale_x_continuous(
    label = percent_format()
  ) +
  scale_y_continuous(
    label = percent_format()
  ) +
  scale_size_continuous(
    label = number_format(big.mark = ",", accuracy = 1)
  ) +
  theme_minimal() +
  theme(
    legend.position = "top"
  ) +
  NULL




```


It´s necessary to verify if the positive correlation that we found is statistically significant. Table \ref{tab_reg} shows the results of these regressions. In the form:

$$ NewAlertsProportion = \alpha + \beta NewCommentsProportions $$

and

$$ AlertsNormalizedDifferences = \alpha + \beta AlertsNormalizedDifferences $$



As we can see by the P-Value of the betas, we cannot reject the null hypothesis in which there is no relation between comments and alerts.




```{r}

library(parsnip)
library(gtsummary)

lm_prop <-  linear_reg() %>% 
  set_engine("lm")

lm_prop_fit <- lm_prop %>% 
  fit(prop_new_alerts ~ prop_new_comments, data = comparisons %>% filter(friction > 1000))

lm_prop_friction <-  linear_reg() %>% 
  set_engine("lm")

lm_prop_fit_friction <- lm_prop %>% 
  fit(prop_new_alerts_friction ~ prop_new_comments_friction, data = comparisons %>% filter(friction > 1000))


tbl_prop <- tbl_regression(
  lm_prop_fit$fit,
  pvalue_fun = function(x) style_pvalue(x, digits = 2),
  label = prop_new_comments ~ "New Comments Proportion",
  intercept = TRUE
) 

tbl_norm <- tbl_regression(
  lm_prop_fit_friction$fit,
  pvalue_fun = function(x) style_pvalue(x, digits = 2),
  label = prop_new_comments_friction ~ "Comments Norm. Difference",
  intercept = TRUE
) 

tbl_final <- tbl_merge(
  list(tbl_prop, tbl_norm),
  tab_spanner = c("New Alerts Proportions","Alerts Norm. Differences")
)


tbl_final %>% 
  as_kable_extra(caption = "\\label{tab_reg} Regression: alerts on comments")




```


In order to be able to reject the null hypothesis and accept that there is a correlation between comments and alerts, we must run the same procedures for more versions of the project and for more projects. We can refine the way we select the comments, too. There are papers that use more sofisticated schemes to identify SATD comments. These can be our next steps. 



\section{References}


